\subsection{Cache}

As already said, providing caches for physical memory is optional in MMIX. Therefore, it can be implemented in nearly arbitrary ways or not at all.

\subsubsection{Organisation}

For GIMMIX, it has been decided to keep it as simple as possible, but complicated enough to be able to implement all instructions regarding caches in a sensible way. In short, GIMMIX provides a fully associative, write-back, write-alloc cache and with a random replacement policy. Each cache consists of a specific number of \i{cache blocks} (also known as \i{cache lines}), whereas each cache block contains a specific number of octas and has a dirty flag. Additionally, each cache block has a \i{tag}, which corresponds to the physical address of the first octa in it. More detailed:
\begin{itemize}
	\item \i{Fully associative} means, that every address can be put into every cache slot. Thus, all slots will be searched if an address is looked up. It has been chosen for simplicity.
	\item \i{Write-back} means, that the cache content is not immediatly written back to main memory, but only as soon as necessary or explicitly requested. Without it, the OS would not have to make sure that the main memory is up to date, if for example instructions were loaded into memory (in privileged mode, a \mi{SYNCID} removes blocks from the caches without writing it to main memory first. Thus, without write-back, a \mi{SYNCID} would be enough; otherwise, a \mi{SYNCD} has to be done first).
	\item The term \i{write-alloc} means, that if writing and the associated cache block of an address it not yet in cache, the block will be read from memory into the cache first and the value will be put into that block. The advantage in this case is, that it makes it more transparent and consistent, because all data is always at first in the cache (if not explicitly requested otherwise).
	\item A random replacement policy has been chosen to make the selection of victims simple. Random in this case means, that the module will walk through the slots in linear order, \ie 0, 1, \dots, $n-1$, 0, 1, \dots, if $n$ is the number of slots.
\end{itemize}

The module \i{cache} provides an instruction and a data cache and offers functions for reading/writing from/to a specific physical address, removing blocks and flushing blocks to memory. All these functions refer to the cache, that has been specified via parameter (either instruction or data cache).

\subsubsection{Reading and Writing}

The functions for reading and writing are the most interesting ones and are thus explained in more detail. The function `cache_read` is implemented as follows:
\begin{lstlisting}[language=GIMMIXC,caption={Implementation of {\tt cache\_read}}]
octa cache_read(int cache,octa addr,int flags) {
	if(addr >= DEV_START_ADDR)
		return bus_read(addr,flags & MEM_SIDE_EFFECTS);

	sCache *c = caches + cache;
	sCacheBlock *block = cache_get(c,addr,flags,false);
	if(!block)
		return bus_read(addr,flags & MEM_SIDE_EFFECTS);
	return block->data[(addr & ~c->tagMask) / sizeof(octa)];
}
\end{lstlisting}
At first, it is checked whether the physical address refers to an I/O device, whose area starts at `DEV_START_ADDR` (defined as \haddro{0001}{0000}{0000}{0000}). I/O devices are always uncached and thus, `cache_read` directly reads from the bus, ignoring the cache. If main memory is requested, `cache_get` will be used to get the cache block for the specified address. By default, the value will be extracted from the corresponding position and returned. But there are cases, that require a direct access of the bus. For example, when no side effects are desired or `flags` contains `MEM_UNCACHED`.

The function to write to the cache is implemented analogously, except that it calls `bus_write` and sets the corresponding octa in the cache block to the specified value. Both functions use `cache_get` to determine the affected cache block, which should be described in more detail:
\begin{lstlisting}[language=GIMMIXC,caption={Implementation of {\tt cache\_get}}]
static sCacheBlock *cache_get(sCache *cache,octa addr,int flags,bool isWrite) {
	sCacheBlock *block = cache_find(cache,addr);
	if(flags & MEM_SIDE_EFFECTS)
		ev_fire2(EV_CACHE_LOOKUP,cache - caches,addr);

	if(block == NULL) {
		if(!(flags & MEM_UNCACHED) &&
				(isWrite || (flags & MEM_SIDE_EFFECTS))) {
			block = cache_findVictim(cache);
			if(block) {
				if(block->dirty)
					cache_flushBlock(cache,block,flags);
				if(!(flags & MEM_UNINITIALIZED))
					cache_fill(cache,block,addr,flags);
				else {
					block->tag = addr & cache->tagMask;
					memset(block->data,0,cache->blockSize);
				}
			}
		}
	}
	return block;
}
\end{lstlisting}
At first, `cache_find` is called, which simply iterates through all cache blocks and returns the block, if it is already present. If it is not yet present, it might have to be loaded first. As the listing shows, this will not be done, if `MEM_UNCACHED` is desired and otherwise always when writing to the cache, but for reading only, if side effects are tolerated. The idea behind it is, that writes -- in contrary to reads -- are considered as an explicit request to change the state of MMIX. That means, if the CLI is used to set a value in memory, for example, the state will be changed. Instead, reading in the CLI does never produce any side effect. If the block has to be loaded, a free block will be required. The function `cache_findVictim` will choose a free one or an arbitrary victim. This does always succeed, except if `cache` has no cache blocks at all (that is, the cache is disabled, which can be configured). If a block has been found, it will have to be flushed to memory, if it is dirty. Finally, it is either filled with zeros or filled with the values from the corresponding memory location.

\subsubsection{Implementation of Caching Instructions}

Last but not least, it should be explained how the instructions regarding caching are implemented in GIMMIX. Because the MMIX architecture does not specify that completely, but leaves some details up to the particular implementation.

\begin{itemize}
	\item \mi{LDUNC \$X,\$Y,\$Z|Z} behaves as \mi{LDO}, but passes `MEM_UNCACHED` to the memory hierarchy to request that the associated cache block is not loaded from memory, if not already present.
	\item \mi{STUNC \$X,\$Y,\$Z|Z} behaves as \mi{STO}, but does also use the flag `MEM_UNCACHED`.
	\item \mi{PRELD X,\$Y,\$Z|Z} ensures, that the bytes \vmem{1}{\dr{Y} + \udrim{Z}}, \dots, \vmem{1}{\dr{Y} + \udrim{Z} + {\tt X}} are present in the data cache.
	\item \mi{PREGO X,\$Y,\$Z|Z} has the same behaviour, but puts the bytes in that range into the instruction cache.
	\item \mi{PREST X,\$Y,\$Z|Z} makes sure, that the bytes \vmem{1}{\dr{Y} + \udrim{Z}}, \dots, \vmem{1}{\dr{Y} + \udrim{Z} + {\tt X}} are in cache, but passes `MEM_UNINITIALIZED` to the memory hierarchy. This way, the data is not loaded from memory, but initialized with zeros (which is of course not required, but simplifies debugging).
	\item \mi{SYNCD X,\$Y,\$Z|Z} flushes the cache blocks affected by the range \vmem{1}{\dr{Y} + \udrim{Z}}, \dots, \vmem{1}{\dr{Y} + \udrim{Z} + {\tt X}} to main memory. If running in privileged mode, they will additionally be removed from the cache.
	\item \mi{SYNCID X,\$Y,\$Z|Z} will remove the specified range from the instruction cache and flushes the range in the data cache, if running in user mode. This way, manually fabricated instructions will be interpreted correctly, because they have to be loaded from main memory into the instruction cache first and the content of the data cache has been flushed to main memory. If running in privileged mode, the range will be removed from both the instruction and the data cache.
\end{itemize}
The instructions \mi{PRELD}, \mi{PREGO} and \mi{PREST} use `MEM_IGNORE_PROT_EX` to ignore protection faults. Analogously, \mi{SYNCD} and \mi{SYNCID} catch these \glslink{Exception}{exceptions} to ensure, that no protection fault is triggered.

